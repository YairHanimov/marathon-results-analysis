{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IES - Python\n",
    "## Topic Proposal\n",
    "## Prague Marathon Results\n",
    "#### David Koubek, Jiri Zelenka\n",
    "28 March 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We propose scraping the RUNCZECH website for results of marathon runners. The scraping would essentially consist of retrieving the table of results (times of runners etc.) which is spread out across various pages, and across various years of marathon events. The data analysis could delve into how the distributions of finishing times of runners varies across past 20 years or so. We expect the finishing times to be improving over past decades. Different visualisation techniques are suitable here for the cross-section data that also contain e.g. the nationalities. Graphing coloured maps with respect to average time of the nation, or across Czech cities for other than Prague marathons.\n",
    "\n",
    "\n",
    "#### Scraping procedure\n",
    " - use BeautifulSoup package among others, for creation of a soup variable containing the required html code\n",
    " - in \"Results\" menu (https://www.runczech.com/srv/www/qf/en/ramjet/results/list) of the website, scrape all marathon weekend links on all 7 pages and save their urls. It should be say 20 links for 20 years of marathon results. The links would look e.g. like https://www.runczech.com/srv/www/qf/en/ramjet/resultsEventDetail?eventId=21429 . The links would be found in <a> elements using soup.findAll('a',{'class':'indexList_link'})\n",
    " - for each of the 20 links (for each marathon year), scrape the hundreds of pages of table rows and save the data. The table of results for a marathon is structured in <tr> rows which we'd save to a dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The contribution of this topic is in further interesting analyses performed than provided by the RUNCZECH website. Getting the data from the website is not very simple without scraping techniques, as they allow just 15 rows of results displayed on a page, and more importantly there is no option of downloading the whole dataset in e.g. CSV or anything. So scraping is the only option here."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
